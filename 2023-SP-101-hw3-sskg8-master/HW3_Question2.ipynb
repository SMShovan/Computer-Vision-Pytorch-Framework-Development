{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agA2OphbEJNR"
      },
      "source": [
        "## Section 1: Import Libraries and load cifar10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i2q9GBaJEOKg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "#import additional libraries if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ejDqWAoU4806"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_data(batch_size):\n",
        "# load both training and test datasets, and transform them to tensors.\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                          download=True, transform=transform)\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "  \n",
        "  return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA4vxj85yiPL",
        "outputId": "e3095b29-9964-410d-de3a-dedbfb27d73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 46611553.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Image batch dimensions: torch.Size([4, 3, 224, 224])\n",
            "Image label dimensions: torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "trainloader, testloader = load_cifar10_data(batch_size)\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "## Check the dimensions of a batch:\n",
        "for images, labels in trainloader:  \n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7K5RvMDAECm"
      },
      "source": [
        "## Section 2: Load Pre-trained Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNGs3uNj-3Kg",
        "outputId": "a18bb36d-0044-4563-f11b-9bf0ab043ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 184MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 236MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 197MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "model0 = models.alexnet(pretrained=True)\n",
        "model1 = models.vgg16(pretrained=True)\n",
        "model2 = models.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "# Freeze all pre-trained layers\n",
        "for param in model0.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Replace the output layer with your own output layer\n",
        "model0.classifier[4] = nn.Linear(4096,1024)\n",
        "model0.classifier[6] = nn.Linear(1024,10)\n",
        "\n",
        "# Unfreeze the changed layers\n",
        "for param in model0.classifier[4].parameters(): #changed layers\n",
        "  param.requires_grad = True\n",
        "for param in model0.classifier[6].parameters(): \n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "#for vgg\n",
        "\n",
        "for param in model1.parameters():\n",
        "  param.requires_grad = False\n",
        "model1.classifier[3] = nn.Linear(4096,1024)\n",
        "model1.classifier[6] = nn.Linear(1024,10)\n",
        "for param in model1.classifier[3].parameters(): \n",
        "  param.requires_grad = True\n",
        "for param in model1.classifier[6].parameters(): \n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "#for ResNet-50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G0qB_wH7yiPN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "for param in model2.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# model2.fc = nn.Linear(2048, 10)\n",
        "# new_fc = nn.Linear(1000, 10)\n",
        "# # Connect the layers\n",
        "# model2 = nn.Sequential(\n",
        "#     model2.fc,\n",
        "#     new_fc\n",
        "# )\n",
        "\n",
        "model2.fc = nn.Sequential(\n",
        "               nn.Linear(2048, 128),\n",
        "               nn.ReLU(inplace=True),\n",
        "               nn.Linear(128, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGpCTfhPyiPO"
      },
      "source": [
        "## Section 3: Define optimizer and loss criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uRU82qbjyiPO"
      },
      "outputs": [],
      "source": [
        "#TODO: Declare your hyperparameters here\n",
        "random_seed = 20\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# optimizer\n",
        "optimizer0 = optim.SGD(model0.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer1 = optim.SGD(model1.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=learning_rate, momentum=0.9)\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVh4UG3NyiPO",
        "outputId": "4a0330c2-8a56-4db2-d1e3-4095ab54b00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:', DEVICE)\n",
        "# Set my pretrained model to be run on GPU\n",
        "model0 = model0.to(DEVICE)\n",
        "model1 = model1.to(DEVICE)\n",
        "model2 = model2.to(DEVICE)\n",
        "criterion.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-tPVp_IyiPP"
      },
      "source": [
        "## Section 4: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uAcy39M0yiPP"
      },
      "outputs": [],
      "source": [
        "def train_model(model,num_epochs, trainloader, optimizer, criterion):\n",
        "    for epoch in range(num_epochs):\n",
        "    \n",
        "        model= model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(trainloader):\n",
        "            features = features.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            \n",
        "            # Forward and backward propagation\n",
        "            optimizer.zero_grad()\n",
        "            output = model(features)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print loss and accuracy every 10 batches\n",
        "            if batch_idx % 1000 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, batch_idx, len(trainloader), loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KS4Ao86nyiPQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yatnjDfvyiPQ"
      },
      "source": [
        "## Section 5: Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiO8SIFayiPQ"
      },
      "source": [
        "Write a function to test the model using testloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V25yJxaXyiPR"
      },
      "outputs": [],
      "source": [
        "def test_model(testloader, model):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize counters for correct predictions and total examples\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over the dataloader\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Move the data to the device\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update counters\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    #print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f} %')\n",
        "    print('Accuracy of the network on the test images for model:  %.2f %%' % (accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcScMucLyiPS",
        "outputId": "2432b661-6f1a-473c-e204-a45803204c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [0/12500], Loss: 2.2539\n",
            "Epoch [1/20], Step [1000/12500], Loss: 1.8329\n",
            "Epoch [1/20], Step [2000/12500], Loss: 1.5316\n",
            "Epoch [1/20], Step [3000/12500], Loss: 0.8016\n",
            "Epoch [1/20], Step [4000/12500], Loss: 0.5727\n",
            "Epoch [1/20], Step [5000/12500], Loss: 0.6139\n",
            "Epoch [1/20], Step [6000/12500], Loss: 1.1881\n",
            "Epoch [1/20], Step [7000/12500], Loss: 2.0873\n",
            "Epoch [1/20], Step [8000/12500], Loss: 1.0095\n",
            "Epoch [1/20], Step [9000/12500], Loss: 1.0340\n",
            "Epoch [1/20], Step [10000/12500], Loss: 0.1783\n",
            "Epoch [1/20], Step [11000/12500], Loss: 0.6847\n",
            "Epoch [1/20], Step [12000/12500], Loss: 1.3464\n",
            "Epoch [2/20], Step [0/12500], Loss: 0.9374\n",
            "Epoch [2/20], Step [1000/12500], Loss: 0.5692\n",
            "Epoch [2/20], Step [2000/12500], Loss: 0.8197\n",
            "Epoch [2/20], Step [3000/12500], Loss: 2.8599\n",
            "Epoch [2/20], Step [4000/12500], Loss: 1.8667\n",
            "Epoch [2/20], Step [5000/12500], Loss: 0.6171\n",
            "Epoch [2/20], Step [6000/12500], Loss: 0.7177\n",
            "Epoch [2/20], Step [7000/12500], Loss: 1.8517\n",
            "Epoch [2/20], Step [8000/12500], Loss: 0.0452\n",
            "Epoch [2/20], Step [9000/12500], Loss: 1.5511\n",
            "Epoch [2/20], Step [10000/12500], Loss: 0.5283\n",
            "Epoch [2/20], Step [11000/12500], Loss: 0.4539\n",
            "Epoch [2/20], Step [12000/12500], Loss: 1.0571\n",
            "Epoch [3/20], Step [0/12500], Loss: 1.3840\n",
            "Epoch [3/20], Step [1000/12500], Loss: 0.2896\n",
            "Epoch [3/20], Step [2000/12500], Loss: 1.4764\n",
            "Epoch [3/20], Step [3000/12500], Loss: 1.6355\n",
            "Epoch [3/20], Step [4000/12500], Loss: 0.9220\n",
            "Epoch [3/20], Step [5000/12500], Loss: 0.2781\n",
            "Epoch [3/20], Step [6000/12500], Loss: 0.9184\n",
            "Epoch [3/20], Step [7000/12500], Loss: 0.3320\n",
            "Epoch [3/20], Step [8000/12500], Loss: 0.6603\n",
            "Epoch [3/20], Step [9000/12500], Loss: 1.4563\n",
            "Epoch [3/20], Step [10000/12500], Loss: 0.7377\n",
            "Epoch [3/20], Step [11000/12500], Loss: 0.4915\n",
            "Epoch [3/20], Step [12000/12500], Loss: 0.6944\n",
            "Epoch [4/20], Step [0/12500], Loss: 1.3850\n",
            "Epoch [4/20], Step [1000/12500], Loss: 0.4518\n",
            "Epoch [4/20], Step [2000/12500], Loss: 0.9701\n",
            "Epoch [4/20], Step [3000/12500], Loss: 0.7210\n",
            "Epoch [4/20], Step [4000/12500], Loss: 2.2996\n",
            "Epoch [4/20], Step [5000/12500], Loss: 0.2977\n",
            "Epoch [4/20], Step [6000/12500], Loss: 1.1580\n",
            "Epoch [4/20], Step [7000/12500], Loss: 0.9760\n",
            "Epoch [4/20], Step [8000/12500], Loss: 0.1943\n",
            "Epoch [4/20], Step [9000/12500], Loss: 0.3856\n",
            "Epoch [4/20], Step [10000/12500], Loss: 0.5751\n",
            "Epoch [4/20], Step [11000/12500], Loss: 0.4381\n",
            "Epoch [4/20], Step [12000/12500], Loss: 0.8238\n",
            "Epoch [5/20], Step [0/12500], Loss: 1.0648\n",
            "Epoch [5/20], Step [1000/12500], Loss: 1.1809\n",
            "Epoch [5/20], Step [2000/12500], Loss: 0.6549\n",
            "Epoch [5/20], Step [3000/12500], Loss: 1.3684\n",
            "Epoch [5/20], Step [4000/12500], Loss: 2.0197\n",
            "Epoch [5/20], Step [5000/12500], Loss: 0.6816\n",
            "Epoch [5/20], Step [6000/12500], Loss: 1.4188\n",
            "Epoch [5/20], Step [7000/12500], Loss: 0.7960\n",
            "Epoch [5/20], Step [8000/12500], Loss: 0.3648\n",
            "Epoch [5/20], Step [9000/12500], Loss: 0.2364\n",
            "Epoch [5/20], Step [10000/12500], Loss: 1.9977\n",
            "Epoch [5/20], Step [11000/12500], Loss: 0.6018\n",
            "Epoch [5/20], Step [12000/12500], Loss: 1.8908\n",
            "Epoch [6/20], Step [0/12500], Loss: 0.9679\n",
            "Epoch [6/20], Step [1000/12500], Loss: 1.5105\n",
            "Epoch [6/20], Step [2000/12500], Loss: 1.6736\n",
            "Epoch [6/20], Step [3000/12500], Loss: 2.6079\n",
            "Epoch [6/20], Step [4000/12500], Loss: 0.3777\n",
            "Epoch [6/20], Step [5000/12500], Loss: 2.8475\n",
            "Epoch [6/20], Step [6000/12500], Loss: 1.0843\n",
            "Epoch [6/20], Step [7000/12500], Loss: 2.2880\n",
            "Epoch [6/20], Step [8000/12500], Loss: 1.0983\n",
            "Epoch [6/20], Step [9000/12500], Loss: 0.3543\n",
            "Epoch [6/20], Step [10000/12500], Loss: 0.0280\n",
            "Epoch [6/20], Step [11000/12500], Loss: 0.3313\n",
            "Epoch [6/20], Step [12000/12500], Loss: 0.2683\n",
            "Epoch [7/20], Step [0/12500], Loss: 2.4836\n",
            "Epoch [7/20], Step [1000/12500], Loss: 0.4298\n",
            "Epoch [7/20], Step [2000/12500], Loss: 0.7820\n",
            "Epoch [7/20], Step [3000/12500], Loss: 3.0219\n",
            "Epoch [7/20], Step [4000/12500], Loss: 0.7446\n",
            "Epoch [7/20], Step [5000/12500], Loss: 1.7148\n",
            "Epoch [7/20], Step [6000/12500], Loss: 0.0149\n",
            "Epoch [7/20], Step [7000/12500], Loss: 1.1490\n",
            "Epoch [7/20], Step [8000/12500], Loss: 0.5862\n",
            "Epoch [7/20], Step [9000/12500], Loss: 0.1421\n",
            "Epoch [7/20], Step [10000/12500], Loss: 1.6614\n",
            "Epoch [7/20], Step [11000/12500], Loss: 1.6272\n",
            "Epoch [7/20], Step [12000/12500], Loss: 0.7888\n",
            "Epoch [8/20], Step [0/12500], Loss: 0.1984\n",
            "Epoch [8/20], Step [1000/12500], Loss: 1.7980\n",
            "Epoch [8/20], Step [2000/12500], Loss: 0.5693\n",
            "Epoch [8/20], Step [3000/12500], Loss: 0.6544\n",
            "Epoch [8/20], Step [4000/12500], Loss: 1.4489\n",
            "Epoch [8/20], Step [5000/12500], Loss: 2.3408\n",
            "Epoch [8/20], Step [6000/12500], Loss: 0.1267\n",
            "Epoch [8/20], Step [7000/12500], Loss: 0.9504\n",
            "Epoch [8/20], Step [8000/12500], Loss: 0.3064\n",
            "Epoch [8/20], Step [9000/12500], Loss: 0.3239\n",
            "Epoch [8/20], Step [10000/12500], Loss: 0.1194\n",
            "Epoch [8/20], Step [11000/12500], Loss: 0.8167\n",
            "Epoch [8/20], Step [12000/12500], Loss: 1.5974\n",
            "Epoch [9/20], Step [0/12500], Loss: 0.7017\n",
            "Epoch [9/20], Step [1000/12500], Loss: 1.6272\n",
            "Epoch [9/20], Step [2000/12500], Loss: 0.6083\n",
            "Epoch [9/20], Step [3000/12500], Loss: 0.8856\n",
            "Epoch [9/20], Step [4000/12500], Loss: 0.3188\n",
            "Epoch [9/20], Step [5000/12500], Loss: 1.1153\n",
            "Epoch [9/20], Step [6000/12500], Loss: 0.3446\n",
            "Epoch [9/20], Step [7000/12500], Loss: 0.8982\n",
            "Epoch [9/20], Step [8000/12500], Loss: 1.3432\n",
            "Epoch [9/20], Step [9000/12500], Loss: 1.1650\n",
            "Epoch [9/20], Step [10000/12500], Loss: 0.2766\n",
            "Epoch [9/20], Step [11000/12500], Loss: 0.2370\n",
            "Epoch [9/20], Step [12000/12500], Loss: 1.5558\n",
            "Epoch [10/20], Step [0/12500], Loss: 1.0295\n",
            "Epoch [10/20], Step [1000/12500], Loss: 0.9851\n",
            "Epoch [10/20], Step [2000/12500], Loss: 0.0526\n",
            "Epoch [10/20], Step [3000/12500], Loss: 1.1358\n",
            "Epoch [10/20], Step [4000/12500], Loss: 2.4345\n",
            "Epoch [10/20], Step [5000/12500], Loss: 1.6893\n",
            "Epoch [10/20], Step [6000/12500], Loss: 0.7693\n",
            "Epoch [10/20], Step [7000/12500], Loss: 1.4127\n",
            "Epoch [10/20], Step [8000/12500], Loss: 1.1960\n",
            "Epoch [10/20], Step [9000/12500], Loss: 1.6007\n",
            "Epoch [10/20], Step [10000/12500], Loss: 1.0644\n",
            "Epoch [10/20], Step [11000/12500], Loss: 1.8387\n",
            "Epoch [10/20], Step [12000/12500], Loss: 0.1887\n",
            "Epoch [11/20], Step [0/12500], Loss: 2.0564\n",
            "Epoch [11/20], Step [1000/12500], Loss: 0.5982\n",
            "Epoch [11/20], Step [2000/12500], Loss: 0.9716\n",
            "Epoch [11/20], Step [3000/12500], Loss: 0.4171\n",
            "Epoch [11/20], Step [4000/12500], Loss: 0.9734\n",
            "Epoch [11/20], Step [5000/12500], Loss: 0.1409\n",
            "Epoch [11/20], Step [6000/12500], Loss: 1.0005\n",
            "Epoch [11/20], Step [7000/12500], Loss: 0.9044\n",
            "Epoch [11/20], Step [8000/12500], Loss: 0.1024\n",
            "Epoch [11/20], Step [9000/12500], Loss: 0.3056\n",
            "Epoch [11/20], Step [10000/12500], Loss: 0.0526\n",
            "Epoch [11/20], Step [11000/12500], Loss: 0.7250\n",
            "Epoch [11/20], Step [12000/12500], Loss: 0.8460\n",
            "Epoch [12/20], Step [0/12500], Loss: 0.9962\n",
            "Epoch [12/20], Step [1000/12500], Loss: 1.4265\n",
            "Epoch [12/20], Step [2000/12500], Loss: 1.3590\n",
            "Epoch [12/20], Step [3000/12500], Loss: 0.8433\n",
            "Epoch [12/20], Step [4000/12500], Loss: 0.5454\n",
            "Epoch [12/20], Step [5000/12500], Loss: 0.1747\n",
            "Epoch [12/20], Step [6000/12500], Loss: 0.9014\n",
            "Epoch [12/20], Step [7000/12500], Loss: 0.3281\n",
            "Epoch [12/20], Step [8000/12500], Loss: 1.9844\n",
            "Epoch [12/20], Step [9000/12500], Loss: 0.9968\n",
            "Epoch [12/20], Step [10000/12500], Loss: 3.6614\n",
            "Epoch [12/20], Step [11000/12500], Loss: 0.2418\n",
            "Epoch [12/20], Step [12000/12500], Loss: 0.1982\n",
            "Epoch [13/20], Step [0/12500], Loss: 0.4919\n",
            "Epoch [13/20], Step [1000/12500], Loss: 1.2402\n",
            "Epoch [13/20], Step [2000/12500], Loss: 1.1720\n",
            "Epoch [13/20], Step [3000/12500], Loss: 0.7991\n",
            "Epoch [13/20], Step [4000/12500], Loss: 1.3736\n",
            "Epoch [13/20], Step [5000/12500], Loss: 1.3904\n",
            "Epoch [13/20], Step [6000/12500], Loss: 1.0647\n",
            "Epoch [13/20], Step [7000/12500], Loss: 1.6167\n",
            "Epoch [13/20], Step [8000/12500], Loss: 0.3212\n",
            "Epoch [13/20], Step [9000/12500], Loss: 0.8761\n",
            "Epoch [13/20], Step [10000/12500], Loss: 0.8107\n",
            "Epoch [13/20], Step [11000/12500], Loss: 0.2849\n",
            "Epoch [13/20], Step [12000/12500], Loss: 0.1455\n",
            "Epoch [14/20], Step [0/12500], Loss: 0.7848\n",
            "Epoch [14/20], Step [1000/12500], Loss: 0.3999\n",
            "Epoch [14/20], Step [2000/12500], Loss: 0.4389\n",
            "Epoch [14/20], Step [3000/12500], Loss: 3.6305\n",
            "Epoch [14/20], Step [4000/12500], Loss: 1.3062\n",
            "Epoch [14/20], Step [5000/12500], Loss: 0.8527\n",
            "Epoch [14/20], Step [6000/12500], Loss: 0.6812\n",
            "Epoch [14/20], Step [7000/12500], Loss: 1.6767\n",
            "Epoch [14/20], Step [8000/12500], Loss: 2.1016\n",
            "Epoch [14/20], Step [9000/12500], Loss: 1.1995\n",
            "Epoch [14/20], Step [10000/12500], Loss: 0.0061\n",
            "Epoch [14/20], Step [11000/12500], Loss: 0.6890\n",
            "Epoch [14/20], Step [12000/12500], Loss: 0.9536\n",
            "Epoch [15/20], Step [0/12500], Loss: 0.2808\n",
            "Epoch [15/20], Step [1000/12500], Loss: 0.8855\n",
            "Epoch [15/20], Step [2000/12500], Loss: 0.8282\n",
            "Epoch [15/20], Step [3000/12500], Loss: 0.2853\n",
            "Epoch [15/20], Step [4000/12500], Loss: 0.8457\n",
            "Epoch [15/20], Step [5000/12500], Loss: 0.7192\n",
            "Epoch [15/20], Step [6000/12500], Loss: 0.2236\n",
            "Epoch [15/20], Step [7000/12500], Loss: 0.6768\n",
            "Epoch [15/20], Step [8000/12500], Loss: 1.2885\n",
            "Epoch [15/20], Step [9000/12500], Loss: 0.3944\n",
            "Epoch [15/20], Step [10000/12500], Loss: 0.5657\n",
            "Epoch [15/20], Step [11000/12500], Loss: 0.2365\n",
            "Epoch [15/20], Step [12000/12500], Loss: 0.0013\n",
            "Epoch [16/20], Step [0/12500], Loss: 0.0750\n",
            "Epoch [16/20], Step [1000/12500], Loss: 0.7775\n",
            "Epoch [16/20], Step [2000/12500], Loss: 0.2375\n",
            "Epoch [16/20], Step [3000/12500], Loss: 0.4920\n",
            "Epoch [16/20], Step [4000/12500], Loss: 1.1571\n",
            "Epoch [16/20], Step [5000/12500], Loss: 0.8990\n",
            "Epoch [16/20], Step [6000/12500], Loss: 0.8973\n",
            "Epoch [16/20], Step [7000/12500], Loss: 0.1336\n",
            "Epoch [16/20], Step [8000/12500], Loss: 2.1145\n",
            "Epoch [16/20], Step [9000/12500], Loss: 0.7492\n",
            "Epoch [16/20], Step [10000/12500], Loss: 2.1556\n",
            "Epoch [16/20], Step [11000/12500], Loss: 0.8371\n",
            "Epoch [16/20], Step [12000/12500], Loss: 0.3474\n",
            "Epoch [17/20], Step [0/12500], Loss: 0.9211\n",
            "Epoch [17/20], Step [1000/12500], Loss: 0.1785\n",
            "Epoch [17/20], Step [2000/12500], Loss: 0.3281\n",
            "Epoch [17/20], Step [3000/12500], Loss: 0.5510\n",
            "Epoch [17/20], Step [4000/12500], Loss: 2.4761\n",
            "Epoch [17/20], Step [5000/12500], Loss: 0.6117\n",
            "Epoch [17/20], Step [6000/12500], Loss: 1.3363\n",
            "Epoch [17/20], Step [7000/12500], Loss: 0.0104\n",
            "Epoch [17/20], Step [8000/12500], Loss: 0.5077\n",
            "Epoch [17/20], Step [9000/12500], Loss: 0.9040\n",
            "Epoch [17/20], Step [10000/12500], Loss: 0.1242\n",
            "Epoch [17/20], Step [11000/12500], Loss: 0.6832\n",
            "Epoch [17/20], Step [12000/12500], Loss: 1.0579\n",
            "Epoch [18/20], Step [0/12500], Loss: 0.6778\n",
            "Epoch [18/20], Step [1000/12500], Loss: 1.7157\n",
            "Epoch [18/20], Step [2000/12500], Loss: 0.2807\n",
            "Epoch [18/20], Step [3000/12500], Loss: 1.0815\n",
            "Epoch [18/20], Step [4000/12500], Loss: 1.9396\n",
            "Epoch [18/20], Step [5000/12500], Loss: 0.8256\n",
            "Epoch [18/20], Step [6000/12500], Loss: 0.7002\n",
            "Epoch [18/20], Step [7000/12500], Loss: 1.4232\n",
            "Epoch [18/20], Step [8000/12500], Loss: 0.0753\n",
            "Epoch [18/20], Step [9000/12500], Loss: 0.8184\n",
            "Epoch [18/20], Step [10000/12500], Loss: 0.7040\n",
            "Epoch [18/20], Step [11000/12500], Loss: 0.5835\n",
            "Epoch [18/20], Step [12000/12500], Loss: 0.1912\n",
            "Epoch [19/20], Step [0/12500], Loss: 0.2684\n",
            "Epoch [19/20], Step [1000/12500], Loss: 0.2940\n",
            "Epoch [19/20], Step [2000/12500], Loss: 1.3247\n",
            "Epoch [19/20], Step [3000/12500], Loss: 0.3146\n",
            "Epoch [19/20], Step [4000/12500], Loss: 0.7046\n",
            "Epoch [19/20], Step [5000/12500], Loss: 1.2170\n",
            "Epoch [19/20], Step [6000/12500], Loss: 0.1129\n",
            "Epoch [19/20], Step [7000/12500], Loss: 0.0744\n",
            "Epoch [19/20], Step [8000/12500], Loss: 0.0372\n",
            "Epoch [19/20], Step [9000/12500], Loss: 0.2226\n",
            "Epoch [19/20], Step [10000/12500], Loss: 0.2222\n",
            "Epoch [19/20], Step [11000/12500], Loss: 0.9378\n",
            "Epoch [19/20], Step [12000/12500], Loss: 0.0502\n",
            "Epoch [20/20], Step [0/12500], Loss: 0.7289\n",
            "Epoch [20/20], Step [1000/12500], Loss: 0.1442\n",
            "Epoch [20/20], Step [2000/12500], Loss: 1.2790\n",
            "Epoch [20/20], Step [3000/12500], Loss: 1.1501\n",
            "Epoch [20/20], Step [4000/12500], Loss: 0.4920\n",
            "Epoch [20/20], Step [5000/12500], Loss: 1.2002\n",
            "Epoch [20/20], Step [6000/12500], Loss: 0.2211\n",
            "Epoch [20/20], Step [7000/12500], Loss: 0.4678\n",
            "Epoch [20/20], Step [8000/12500], Loss: 0.3272\n",
            "Epoch [20/20], Step [9000/12500], Loss: 0.9622\n",
            "Epoch [20/20], Step [10000/12500], Loss: 0.4619\n",
            "Epoch [20/20], Step [11000/12500], Loss: 0.5504\n",
            "Epoch [20/20], Step [12000/12500], Loss: 0.3684\n",
            "Accuracy of the network on the test images for model:  74.92 %\n",
            "Epoch [1/20], Step [0/12500], Loss: 2.3333\n",
            "Epoch [1/20], Step [1000/12500], Loss: 0.9763\n",
            "Epoch [1/20], Step [2000/12500], Loss: 0.4640\n",
            "Epoch [1/20], Step [3000/12500], Loss: 0.5040\n",
            "Epoch [1/20], Step [4000/12500], Loss: 0.7241\n",
            "Epoch [1/20], Step [5000/12500], Loss: 0.9699\n",
            "Epoch [1/20], Step [6000/12500], Loss: 1.9885\n",
            "Epoch [1/20], Step [7000/12500], Loss: 0.4429\n",
            "Epoch [1/20], Step [8000/12500], Loss: 1.4664\n",
            "Epoch [1/20], Step [9000/12500], Loss: 0.8946\n",
            "Epoch [1/20], Step [10000/12500], Loss: 0.7720\n",
            "Epoch [1/20], Step [11000/12500], Loss: 1.7576\n",
            "Epoch [1/20], Step [12000/12500], Loss: 0.4603\n",
            "Epoch [2/20], Step [0/12500], Loss: 0.7328\n",
            "Epoch [2/20], Step [1000/12500], Loss: 0.2287\n",
            "Epoch [2/20], Step [2000/12500], Loss: 0.8327\n",
            "Epoch [2/20], Step [3000/12500], Loss: 0.2117\n",
            "Epoch [2/20], Step [4000/12500], Loss: 0.1294\n",
            "Epoch [2/20], Step [5000/12500], Loss: 1.4643\n",
            "Epoch [2/20], Step [6000/12500], Loss: 0.5636\n",
            "Epoch [2/20], Step [7000/12500], Loss: 1.5280\n",
            "Epoch [2/20], Step [8000/12500], Loss: 0.1616\n",
            "Epoch [2/20], Step [9000/12500], Loss: 2.0797\n",
            "Epoch [2/20], Step [10000/12500], Loss: 0.4762\n",
            "Epoch [2/20], Step [11000/12500], Loss: 0.7106\n",
            "Epoch [2/20], Step [12000/12500], Loss: 0.9616\n",
            "Epoch [3/20], Step [0/12500], Loss: 1.4997\n",
            "Epoch [3/20], Step [1000/12500], Loss: 1.0796\n",
            "Epoch [3/20], Step [2000/12500], Loss: 0.5004\n",
            "Epoch [3/20], Step [3000/12500], Loss: 0.0520\n",
            "Epoch [3/20], Step [4000/12500], Loss: 0.5488\n",
            "Epoch [3/20], Step [5000/12500], Loss: 0.0813\n",
            "Epoch [3/20], Step [6000/12500], Loss: 0.5533\n",
            "Epoch [3/20], Step [7000/12500], Loss: 0.2716\n",
            "Epoch [3/20], Step [8000/12500], Loss: 1.6326\n",
            "Epoch [3/20], Step [9000/12500], Loss: 0.2148\n",
            "Epoch [3/20], Step [10000/12500], Loss: 0.9714\n",
            "Epoch [3/20], Step [11000/12500], Loss: 0.8340\n",
            "Epoch [3/20], Step [12000/12500], Loss: 0.6438\n",
            "Epoch [4/20], Step [0/12500], Loss: 0.4114\n",
            "Epoch [4/20], Step [1000/12500], Loss: 0.5843\n",
            "Epoch [4/20], Step [2000/12500], Loss: 0.8577\n",
            "Epoch [4/20], Step [3000/12500], Loss: 0.4660\n",
            "Epoch [4/20], Step [4000/12500], Loss: 1.7516\n",
            "Epoch [4/20], Step [5000/12500], Loss: 0.3627\n",
            "Epoch [4/20], Step [6000/12500], Loss: 1.2038\n",
            "Epoch [4/20], Step [7000/12500], Loss: 3.2190\n",
            "Epoch [4/20], Step [8000/12500], Loss: 0.1841\n",
            "Epoch [4/20], Step [9000/12500], Loss: 1.2724\n",
            "Epoch [4/20], Step [10000/12500], Loss: 0.9304\n",
            "Epoch [4/20], Step [11000/12500], Loss: 0.3246\n",
            "Epoch [4/20], Step [12000/12500], Loss: 0.0008\n",
            "Epoch [5/20], Step [0/12500], Loss: 0.9376\n",
            "Epoch [5/20], Step [1000/12500], Loss: 1.6922\n",
            "Epoch [5/20], Step [2000/12500], Loss: 0.1201\n",
            "Epoch [5/20], Step [3000/12500], Loss: 0.9726\n",
            "Epoch [5/20], Step [4000/12500], Loss: 0.0690\n",
            "Epoch [5/20], Step [5000/12500], Loss: 1.2703\n",
            "Epoch [5/20], Step [6000/12500], Loss: 0.7832\n",
            "Epoch [5/20], Step [7000/12500], Loss: 2.3925\n",
            "Epoch [5/20], Step [8000/12500], Loss: 0.4614\n",
            "Epoch [5/20], Step [9000/12500], Loss: 1.1646\n",
            "Epoch [5/20], Step [10000/12500], Loss: 0.7848\n",
            "Epoch [5/20], Step [11000/12500], Loss: 0.9861\n",
            "Epoch [5/20], Step [12000/12500], Loss: 1.0006\n",
            "Epoch [6/20], Step [0/12500], Loss: 1.4393\n",
            "Epoch [6/20], Step [1000/12500], Loss: 0.1227\n",
            "Epoch [6/20], Step [2000/12500], Loss: 0.9342\n",
            "Epoch [6/20], Step [3000/12500], Loss: 0.7977\n",
            "Epoch [6/20], Step [4000/12500], Loss: 0.8961\n",
            "Epoch [6/20], Step [5000/12500], Loss: 0.6968\n",
            "Epoch [6/20], Step [6000/12500], Loss: 1.6057\n",
            "Epoch [6/20], Step [7000/12500], Loss: 2.1664\n",
            "Epoch [6/20], Step [8000/12500], Loss: 1.4475\n",
            "Epoch [6/20], Step [9000/12500], Loss: 1.1142\n",
            "Epoch [6/20], Step [10000/12500], Loss: 0.4967\n",
            "Epoch [6/20], Step [11000/12500], Loss: 0.0989\n",
            "Epoch [6/20], Step [12000/12500], Loss: 1.0158\n",
            "Epoch [7/20], Step [0/12500], Loss: 0.8816\n",
            "Epoch [7/20], Step [1000/12500], Loss: 0.9448\n",
            "Epoch [7/20], Step [2000/12500], Loss: 2.8989\n",
            "Epoch [7/20], Step [3000/12500], Loss: 0.9385\n",
            "Epoch [7/20], Step [4000/12500], Loss: 2.9617\n",
            "Epoch [7/20], Step [5000/12500], Loss: 0.8785\n",
            "Epoch [7/20], Step [6000/12500], Loss: 0.1102\n",
            "Epoch [7/20], Step [7000/12500], Loss: 1.2962\n",
            "Epoch [7/20], Step [8000/12500], Loss: 0.2856\n",
            "Epoch [7/20], Step [9000/12500], Loss: 1.3458\n",
            "Epoch [7/20], Step [10000/12500], Loss: 0.0702\n",
            "Epoch [7/20], Step [11000/12500], Loss: 1.8233\n",
            "Epoch [7/20], Step [12000/12500], Loss: 0.4774\n",
            "Epoch [8/20], Step [0/12500], Loss: 0.1844\n",
            "Epoch [8/20], Step [1000/12500], Loss: 0.8083\n",
            "Epoch [8/20], Step [2000/12500], Loss: 0.8602\n",
            "Epoch [8/20], Step [3000/12500], Loss: 0.1506\n",
            "Epoch [8/20], Step [4000/12500], Loss: 0.7654\n",
            "Epoch [8/20], Step [5000/12500], Loss: 0.1382\n",
            "Epoch [8/20], Step [6000/12500], Loss: 0.2923\n",
            "Epoch [8/20], Step [7000/12500], Loss: 1.1038\n",
            "Epoch [8/20], Step [8000/12500], Loss: 0.4397\n",
            "Epoch [8/20], Step [9000/12500], Loss: 0.4776\n",
            "Epoch [8/20], Step [10000/12500], Loss: 0.4586\n",
            "Epoch [8/20], Step [11000/12500], Loss: 0.0014\n",
            "Epoch [8/20], Step [12000/12500], Loss: 0.9479\n",
            "Epoch [9/20], Step [0/12500], Loss: 0.0367\n",
            "Epoch [9/20], Step [1000/12500], Loss: 0.0697\n",
            "Epoch [9/20], Step [2000/12500], Loss: 0.7759\n",
            "Epoch [9/20], Step [3000/12500], Loss: 0.7114\n",
            "Epoch [9/20], Step [4000/12500], Loss: 1.2038\n",
            "Epoch [9/20], Step [5000/12500], Loss: 0.3285\n",
            "Epoch [9/20], Step [6000/12500], Loss: 0.1624\n",
            "Epoch [9/20], Step [7000/12500], Loss: 1.1724\n",
            "Epoch [9/20], Step [8000/12500], Loss: 0.9060\n",
            "Epoch [9/20], Step [9000/12500], Loss: 0.0471\n",
            "Epoch [9/20], Step [10000/12500], Loss: 0.2343\n",
            "Epoch [9/20], Step [11000/12500], Loss: 1.8253\n",
            "Epoch [9/20], Step [12000/12500], Loss: 0.2419\n",
            "Epoch [10/20], Step [0/12500], Loss: 1.4343\n",
            "Epoch [10/20], Step [1000/12500], Loss: 0.3422\n",
            "Epoch [10/20], Step [2000/12500], Loss: 0.8956\n",
            "Epoch [10/20], Step [3000/12500], Loss: 0.0399\n",
            "Epoch [10/20], Step [4000/12500], Loss: 0.7509\n",
            "Epoch [10/20], Step [5000/12500], Loss: 0.5542\n",
            "Epoch [10/20], Step [6000/12500], Loss: 0.9502\n",
            "Epoch [10/20], Step [7000/12500], Loss: 0.0350\n",
            "Epoch [10/20], Step [8000/12500], Loss: 3.2090\n",
            "Epoch [10/20], Step [9000/12500], Loss: 0.8756\n",
            "Epoch [10/20], Step [10000/12500], Loss: 0.4580\n",
            "Epoch [10/20], Step [11000/12500], Loss: 1.1942\n",
            "Epoch [10/20], Step [12000/12500], Loss: 0.8144\n",
            "Epoch [11/20], Step [0/12500], Loss: 0.6899\n",
            "Epoch [11/20], Step [1000/12500], Loss: 0.7793\n",
            "Epoch [11/20], Step [2000/12500], Loss: 0.3203\n",
            "Epoch [11/20], Step [3000/12500], Loss: 0.6662\n",
            "Epoch [11/20], Step [4000/12500], Loss: 1.7513\n",
            "Epoch [11/20], Step [5000/12500], Loss: 0.3750\n",
            "Epoch [11/20], Step [6000/12500], Loss: 0.6591\n",
            "Epoch [11/20], Step [7000/12500], Loss: 0.7655\n",
            "Epoch [11/20], Step [8000/12500], Loss: 0.8798\n",
            "Epoch [11/20], Step [9000/12500], Loss: 0.9156\n",
            "Epoch [11/20], Step [10000/12500], Loss: 0.4670\n",
            "Epoch [11/20], Step [11000/12500], Loss: 0.2239\n",
            "Epoch [11/20], Step [12000/12500], Loss: 0.7597\n",
            "Epoch [12/20], Step [0/12500], Loss: 0.1955\n",
            "Epoch [12/20], Step [1000/12500], Loss: 0.1317\n",
            "Epoch [12/20], Step [2000/12500], Loss: 0.5697\n",
            "Epoch [12/20], Step [3000/12500], Loss: 0.7522\n",
            "Epoch [12/20], Step [4000/12500], Loss: 0.6795\n",
            "Epoch [12/20], Step [5000/12500], Loss: 0.9566\n",
            "Epoch [12/20], Step [6000/12500], Loss: 1.0986\n",
            "Epoch [12/20], Step [7000/12500], Loss: 0.4716\n",
            "Epoch [12/20], Step [8000/12500], Loss: 0.5375\n",
            "Epoch [12/20], Step [9000/12500], Loss: 0.8284\n",
            "Epoch [12/20], Step [10000/12500], Loss: 0.4474\n",
            "Epoch [12/20], Step [11000/12500], Loss: 3.5190\n",
            "Epoch [12/20], Step [12000/12500], Loss: 0.0480\n",
            "Epoch [13/20], Step [0/12500], Loss: 1.1027\n",
            "Epoch [13/20], Step [1000/12500], Loss: 0.9141\n",
            "Epoch [13/20], Step [2000/12500], Loss: 0.1354\n",
            "Epoch [13/20], Step [3000/12500], Loss: 0.6896\n",
            "Epoch [13/20], Step [4000/12500], Loss: 0.5433\n",
            "Epoch [13/20], Step [5000/12500], Loss: 3.3632\n",
            "Epoch [13/20], Step [6000/12500], Loss: 0.0495\n",
            "Epoch [13/20], Step [7000/12500], Loss: 0.8382\n"
          ]
        }
      ],
      "source": [
        "train_model(model0,num_epochs, trainloader, optimizer0, criterion)\n",
        "test_model(testloader, model0)\n",
        "\n",
        "train_model(model1,num_epochs, trainloader, optimizer1, criterion)\n",
        "test_model(testloader, model1)\n",
        "\n",
        "train_model(model2,num_epochs, trainloader, optimizer2, criterion)\n",
        "test_model(testloader, model2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPHH4EK00oE4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}